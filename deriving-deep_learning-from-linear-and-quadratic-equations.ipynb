{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting a quadratic function:","metadata":{}},{"cell_type":"code","source":"def quadrtc_fn(x):\n    return 3*x**2 + 2*x + 1\n\nplot_function(quadrtc_fn, \"$3x^2 + 2x + 1$\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imagine we don't know the values of the quadrtc_fn() above. Instead, we are trying to recreate it. This is the general idea of training a model.\n\nLet's see what we can achieve:","metadata":{}},{"cell_type":"code","source":"# Create a general quadratic function\ndef quad(a, b, c, x):\n    return a*x**2 + b*x + c","metadata":{"execution":{"iopub.status.busy":"2023-12-27T09:42:05.913321Z","iopub.execute_input":"2023-12-27T09:42:05.914131Z","iopub.status.idle":"2023-12-27T09:42:05.949533Z","shell.execute_reply.started":"2023-12-27T09:42:05.914097Z","shell.execute_reply":"2023-12-27T09:42:05.948321Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Now let's call the `quad(*params)` function with some random values for parameters:","metadata":{}},{"cell_type":"code","source":"quad(3,2,1, 1.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial # helps make quadratic functions\n\ndef mk_quad(a,b,c):\n    return partial(quad, a,b,c)\n\nf_example = mk_quad(3,2,1)\nf_example(1.5) # -> passing in the value of x cos the others (a,b,c) are already fixed.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit a function by good hands and eyes - Manually changing the parameters a, b, c\nfrom numpy.random import normal, seed, uniform\n\nnp.random.seed(42) # set the seed to ensure we always get the same random numbers\n\ndef noise(x, scale):\n    # create normally distributed random numbers\n    return normal(slace=scale, size=x.shape)\n\ndef add_noise(x, mult, add):\n    return x * (1 + noise(x, mult)) + noise(x, add)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting some data that matches the shape of mk_quad et al functions","metadata":{}},{"cell_type":"code","source":"# plot the quadratic function using\nx = torch.lisepace(-2, 2, steps=20)[:, None] # creates a tensor - a vector going from -2 to 2 in 20 equal steps\ny = add_noise(f_example(x), 0.3, 1.5)\nplt.scatter(x,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have some data.\nWe will try to reconstruct the original quadratic equation. How do we do that?\n_Let's fuck around, and find out_","metadata":{}},{"cell_type":"code","source":"# interact with the parameters to change them\nfrom ipywidgets import interact\n\n@interact(a=1.5, b=1.5, c=1.5)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a, b, c), ylim=(-3, 12))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOW WE AUTOMATE:","metadata":{}},{"cell_type":"markdown","source":"The first step:\nWe need to know, if we increase (or decrease) the value of a parameter, does it get better or worse?","metadata":{}},{"cell_type":"markdown","source":"Introducing the loss functions. These will help with automatically updating the parameters.\n\nLoss function tells us whether the current value of the function and parameters is better or worse than the actual. - relative to making better predictions.","metadata":{}},{"cell_type":"code","source":"# main squared error (mse) - a simple and common loss function\ndef mse(predictions, actuals):\n    return ((predictions - actuals)**2).mean()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the quadratic function with manual parameters but including a display of the loss function\n@interact(a=1.5, b=1.5, c=1.5)\ndef plot_quad(a, b, c):\n    f = mk_quad(a, b, c)\n    plot.scatter(x, y)\n    loss = mse(f(x), y)\n    plot_function(f, ylim=(-3, 12), title=f\"MSE: {loss:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, Automating.\nThe key thing to remember is we want to know, `does the loss get better when we increase or decrease the input?`.\n\nAnother way to see if the loss function is getting better, _other than the manual way of changing the parameters(input),_ is by calculating its **derivative**.\n\nA derivative is a function that tells us, _if we increase the input, does the output increases or decreases, and by how much_. That is called the slope or the gradient.\n\nPytorch calculates the derivative automatically.","metadata":{}},{"cell_type":"code","source":"# Automate the search of parameters for better loss.\n\n# a func that takes coefficients of the quad and returns the mse of predictions and actuals\ndef quad_mse(params):\n    f = mk_quad(*params)\n    return mse(f(x), y) # the loss of the quadratic function\n\nquad_mse([1.5, 1.5, 1.5])\n# returns: tensor(11.4648, dtype=torch.float64) - 1d tensor\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In pytorch, everything is a tensor.\n- 1D tensor - `[1, 2, 3]` - lists and vectors of numbers\n- 2D tensor - rectangles and tables of numbers\n- 3D tensors - Layers of tables of numbers\n- etc","metadata":{}},{"cell_type":"code","source":"# create all the coefficients and put them in a single 1D - rank1 tensor\nabc = torch.tensor([1.5, 1.5, 1.5])\nabc.requires_grad_() # tell pytorch to calculate the gradient of these numbers whenever we use them in a calculation\n\n# return a tensor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use it in a calculation\nloss = quad_mse(abc)\nloss\n# display a tensor with a grad_fn(gradient function)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the gradient\nloss.backward()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# access the calculated gradient\nabc.grad","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the gradient displays a tensor that shows the loss is still big.\n# Now we reduce it\n\nwith torch.no_grad(): # tell pytorch not to calc gradient of abc params\n    abc -= abc.grad*0.01 # updating the gradients\n    loss = quad_mse(abc) # calculate loss\n\nprint(f\"loss={loss:.2f}\") # outputs 10.59","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have update the parameters in one cycle. Let's Automate the whole thing.","metadata":{}},{"cell_type":"code","source":"# The mathematical function\n# we will repeat for five times - This is called Optimization using Gradient descent\nfor i in range(5):\n    loss = quad_mse(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad*0.01\n    \n    print(f\"step={i}; loss={loss:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a basic principle of optimizers in deep learning and ML.\nThis principle of optimization is called _gradient descent_, because we calculate the gradient and the we try to do a descent.\n\nAnd that is it. _*How to find parameters for the model*_ \n\nWe need one more thing. _What is the mathematical function, that we are finding these parameters for?_ We can't just use quadratics, right?","metadata":{}},{"cell_type":"markdown","source":"Enters, **Rectified Linear Function**","metadata":{}},{"cell_type":"code","source":"# infitely flexible function called Rectified Linear Unit\ndef rectified_linear(m,b,x):\n    y = m*x + b # linear function\n    return torch.clip(y, 0.) # if y<0: y = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot it\nplot_function(partial(rectified_linear, 1, 1)) # ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make it interactive:","metadata":{}},{"cell_type":"code","source":"@interact(m=1.5, b=1.5)\ndef plot_relu(m, b): # plot rectified linear unit\n    plot_function(partial(rectified_linear, 1, 1), ylim=(-1,4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add two ReLU together.","metadata":{}},{"cell_type":"code","source":"def double_relu(m1, b1, m2, b2):\n    return rectified_linear(m1, b1, x) + rectified_linear(m2, b2, x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot them\n@interact(m1=1.5, b1=1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1,b1,m2,b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, We can do the same using as extremely many relus, not just two.\n\nWe use gradient descent to get the parameters.\n\nThis is the principle or foundation or the proof or the concept or the deriving of deep learning.\n\nEverything else is just how to make it faster and make it need less data.","metadata":{}},{"cell_type":"markdown","source":"Consider the analogy of how to draw an owl. _You fast draw two circles and then draw the rest of the damn owl._\n\nIn the context of deep learning, Jeremy Howard states that: _\"When you have ReLUs getting added together, and gradient descent to optimize the parameters, and samples of inputs and outputs that you want, the computer draws the owl.\"_","metadata":{}},{"cell_type":"markdown","source":"## Matrix Multiplication\nThe first Case of Optimization and making it easier is **Matrix Multiplication**","metadata":{}},{"cell_type":"markdown","source":"Matrix multiplication is the most critical mathematical operation in basically all of deep learning. It is basically multiplying things together and adding them up.\nIf we have any negatives in the product of the matrix multiplication, we convert it into zero.\n\n- GPUs are good in matrix multiplication, because they have tensor cores which only multiply four by four matrices.","metadata":{}},{"cell_type":"markdown","source":"# # Practice\n- It is time to take on the **Titanic** competition on [Kaggle.com](https://www.kaggle.com/competitions/titanic/data?select=train.csv)\n","metadata":{}},{"cell_type":"markdown","source":"### Next UP\n[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)","metadata":{}}]}